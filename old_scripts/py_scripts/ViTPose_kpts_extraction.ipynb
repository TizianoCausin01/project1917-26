{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b72edc9-f1b7-4955-b7dd-fe4427e9dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial from https://huggingface.co/docs/transformers/en/model_doc/vitpose\n",
    "from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import h5py\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ce063da-0928-4616-92e3-dd5ca7685a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c91b46-4a63-4a54-bdac-067e625c8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_res(res: list, key: str, size: tuple, top_k: int, box=0): \n",
    "    if box == 0:\n",
    "        data = [res[i][key].cpu().numpy() for i in range(len(res))]\n",
    "    else:\n",
    "        data = [np.array([i.cpu().numpy()]) for i in res[\"scores\"]]\n",
    "    if len(data) < top_k: # fills in if there are less than 5 pers\n",
    "        fill_in = [np.full(size, np.nan) for _ in range(int(top_k - len(data)))]\n",
    "        data.extend(fill_in)\n",
    "    \n",
    "    data = data[0:5]\n",
    "    data = np.stack(data, axis=-1) # stacks them along the last dimension\n",
    "    data = data.flatten(order='F') # vectorizes it fortran style (column-major like matlab)\n",
    "    return data \n",
    "# EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9830bfc3-b78d-46cc-b2a0-acb71c929da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:36:48  - frame 1\n",
      "{'scores': tensor([0.5962, 0.5746, 0.4045, 0.3717, 0.3420, 0.3384, 0.3337, 0.3311, 0.3063,\n",
      "        0.3063]), 'labels': tensor([24, 20, 24, 20, 24, 20, 20, 20, 20, 20]), 'boxes': tensor([[ 9.0683e-01,  1.1853e+02,  6.2574e+02,  5.2034e+02],\n",
      "        [ 1.7806e+01, -2.4284e-01,  8.0000e+02,  4.2720e+02],\n",
      "        [ 7.7372e+02,  1.7308e+02,  1.2702e+03,  3.9557e+02],\n",
      "        [ 4.0987e+02,  5.4247e+01,  7.9916e+02,  3.7140e+02],\n",
      "        [ 6.4565e+02,  1.7210e+02,  1.2768e+03,  5.2075e+02],\n",
      "        [ 4.7507e+02,  7.9370e+01,  7.9856e+02,  4.4439e+02],\n",
      "        [ 1.8027e+01,  3.2638e-02,  7.9925e+02,  3.7070e+02],\n",
      "        [ 2.2157e+01,  1.3992e-01,  4.9222e+02,  1.2460e+02],\n",
      "        [ 2.2035e+01,  1.9193e-02,  2.8273e+02,  1.2323e+02],\n",
      "        [ 4.0397e+02,  2.2636e+01,  7.9856e+02,  4.2984e+02]])}\n",
      "tensor([], size=(0, 4))\n",
      "true\n",
      "1\n",
      "18:36:49  - frame 2\n",
      "{'scores': tensor([0.5910, 0.5569, 0.4947, 0.3728, 0.3626, 0.3396, 0.3366, 0.3051, 0.3010]), 'labels': tensor([ 0, 24, 24, 24, 24, 24,  0,  0, 28]), 'boxes': tensor([[ 2.0425e+00, -2.0748e-01,  8.2329e+02,  5.2179e+02],\n",
      "        [ 6.4334e+02,  1.7446e+02,  1.2781e+03,  5.2106e+02],\n",
      "        [ 5.3503e-01,  1.2502e+02,  5.8010e+02,  5.2122e+02],\n",
      "        [ 7.7401e+02,  1.7464e+02,  1.2753e+03,  3.9393e+02],\n",
      "        [ 6.2462e-01,  1.2560e+02,  6.2860e+02,  5.2144e+02],\n",
      "        [ 2.0425e+00, -2.0748e-01,  8.2329e+02,  5.2179e+02],\n",
      "        [ 6.4601e+02,  1.2091e+02,  1.2779e+03,  5.2148e+02],\n",
      "        [ 1.8937e+01, -3.1291e-01,  8.2306e+02,  4.1094e+02],\n",
      "        [ 6.4334e+02,  1.7446e+02,  1.2781e+03,  5.2106e+02]])}\n",
      "tensor([[ 2.0425e+00, -2.0748e-01,  8.2329e+02,  5.2179e+02],\n",
      "        [ 6.4601e+02,  1.2091e+02,  1.2779e+03,  5.2148e+02],\n",
      "        [ 1.8937e+01, -3.1291e-01,  8.2306e+02,  4.1094e+02]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 1 - load video\n",
    "path2mod = \"/Volumes/TIZIANO/models\"\n",
    "\n",
    "# 2 - load models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\") # loads a preprocessing pipeline img (to ensure same preproc.g)\n",
    "person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device) # loads the object detection model:  RT-DETR object detection model (detectiion + label.g -> label 0 = person)\n",
    "#inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device) # preprocesses the image and returns it as a tensor\n",
    "# returns an object of type <class 'transformers.image_processing_base.BatchFeature'> -> behaves like a dict e.g. inputs.keys() -> 'pixel_values' i.e. the normalized img tensor of shape 2, 3, 256, 192\n",
    "image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\") # downloads a processor tailored for the vitpose-base-simple model, it resizes, normalizes, and formats input data (cropping each detected person), automatically includes COCO keypoint configuration.\n",
    "model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device) #downloads ViTPose\n",
    "feats = {\n",
    "    \"kpts\" : [],\n",
    "    \"boxes\" : [], \n",
    "    \"score_boxes\" : [],\n",
    "    \"score_kpts\" : [], \n",
    "    \"head_kpts\" : [],\n",
    "    \"score_heads\" : []\n",
    "}\n",
    "# 3 - read frame and preprocess it\n",
    "runs = [1] #,2,3]\n",
    "for irun in runs:  \n",
    "    path2vid = f\"/Volumes/TIZIANO/stimuli/Project1917_movie_part{irun}_24Hz.mp4\"\n",
    "    reader = cv2.VideoCapture(path2vid)\n",
    "    count = 0\n",
    "    reader.set(cv2.CAP_PROP_POS_FRAMES, 2947)\n",
    "    for i in range(2):\n",
    "        count += 1\n",
    "        print(datetime.now().strftime(\"%H:%M:%S\"),\" - frame\", count, flush=True)\n",
    "        ret, frame = reader.read()\n",
    "        \n",
    "        if ret == False:\n",
    "            break\n",
    "        # end if ret==False:\n",
    "    \n",
    "        frame_rgb = cv2.cvtColor(\n",
    "            frame, cv2.COLOR_BGR2RGB\n",
    "        )  # converts to bgr to rgb color codes\n",
    "    \n",
    "        inputs = person_image_processor(frame_rgb, return_tensors=\"pt\")\n",
    "        \n",
    "    # 4 - detect people\n",
    "        with torch.no_grad():\n",
    "            outputs = person_model(**inputs) # performs object detection on the input\n",
    "    \n",
    "    # 5 - get box predictions\n",
    "        result = person_image_processor.post_process_object_detection(\n",
    "            outputs, target_sizes=torch.tensor([(frame_rgb.shape[0], frame_rgb.shape[1])]), threshold=0.3 # converts raw model outputs into interpretable bounding box predictions \n",
    "        )[0] # selects the first element in the list bc only one img\n",
    "        print(result)\n",
    "        \n",
    "        person_boxes = result[\"boxes\"][result[\"labels\"] == 0] # index only the boxes associated with label 0 (person) in COCO class labels\n",
    "        print(person_boxes)\n",
    "        if person_boxes.numel() == 0: # predef dimensionalities, sorry for hardcoding\n",
    "            person_boxes_store = np.full((20,), np.nan)\n",
    "            score_boxes_store = np.full((5,), np.nan)\n",
    "            kpts_store = np.full((170,), np.nan)\n",
    "            kpts_scores_store = np.full((85,), np.nan)\n",
    "            print(\"true\")\n",
    "        else:\n",
    "            score_boxes = result[\"scores\"][result[\"labels\"] == 0]\n",
    "            score_boxes = score_boxes.cpu().numpy()\n",
    "            # score_boxes_store = fill_in_res(person_boxes, \"scores\", (1, 1), 5) \n",
    "            # feats[\"score_boxes\"].append(score_boxes_store)\n",
    "            # converts boxes from VOC format: (x1, y1, x2, y2) to COCO format: N pers detected x 4 -> 4 cols are => (x, y, width, height)\n",
    "            person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0] \n",
    "            person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1] \n",
    "            \n",
    "        # 6 - preprocess for kpt detection\n",
    "            inputs = image_processor(frame_rgb, boxes=[person_boxes], return_tensors=\"pt\").to(device) # processes the original image using the bounding boxes -> ViTPose expects tightly cropped pics\n",
    "            # inputs is a dict like type with \"pixels_value\" as only entry. It is a tensor [Batch, Channels, Height, Width] -> Batch is the number of people detected\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs) # runs ViTPose\n",
    "            pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])[0]\n",
    "            kpts_store = fill_in_res(pose_results, \"keypoints\", (17,2), 5)\n",
    "            kpts_scores_store = fill_in_res(pose_results, \"scores\", (17), 5)\n",
    "            person_boxes_store = fill_in_res(pose_results, \"bbox\", (4), 5) \n",
    "            score_boxes_store = fill_in_res(result, \"scores\", (1), 5, box=1)\n",
    "        feats[\"boxes\"].append(person_boxes_store) # FIXME it's a list of dicts\n",
    "        feats[\"kpts\"].append(kpts_store)\n",
    "        print(len(feats[\"kpts\"]))\n",
    "        feats[\"score_kpts\"].append(kpts_scores_store)\n",
    "        feats[\"score_boxes\"].append(score_boxes_store)\n",
    "    with h5py.File(f\"{path2mod}/Project1917_dummyViTPose_run0{irun}.h5\", \"w\") as f:\n",
    "        # Iterate over dictionary items and save them in the HDF5 file\n",
    "        for key, value in feats.items():\n",
    "            f.create_dataset(key, data=value)  # Create a dataset for each key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33b2e5e3-48be-4032-a729-b3e4ead51124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats[\"kpts\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df3b9493-e561-4981-af13-911caa95f225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L_Ankle': 15,\n",
       " 'L_Ear': 3,\n",
       " 'L_Elbow': 7,\n",
       " 'L_Eye': 1,\n",
       " 'L_Hip': 11,\n",
       " 'L_Knee': 13,\n",
       " 'L_Shoulder': 5,\n",
       " 'L_Wrist': 9,\n",
       " 'Nose': 0,\n",
       " 'R_Ankle': 16,\n",
       " 'R_Ear': 4,\n",
       " 'R_Elbow': 8,\n",
       " 'R_Eye': 2,\n",
       " 'R_Hip': 12,\n",
       " 'R_Knee': 14,\n",
       " 'R_Shoulder': 6,\n",
       " 'R_Wrist': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1917_py_env",
   "language": "python",
   "name": "1917_py_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
